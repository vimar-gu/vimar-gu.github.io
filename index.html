<!DOCTYPE html>
<html lang="en">
<head>

    <!-- Global site tag (gtag.js) - Google Analytics -->

    <title>Jianyang Gu's Page</title>

    <meta charset="utf-8">
    <!-- <link href='https://fonts.googleapis.com/css?family=Inconsolata:400,700' rel='stylesheet' type='text/css'> -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="viewport" content="initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <link href="./css/index-mobile.css" rel="stylesheet" type="text/css" media="screen and (max-width: 1000px)">
    <link href="./css/index.css" rel="stylesheet" type="text/css" media="screen and (min-width: 1000px)">

    <script type="text/javascript">
        window.onload = function () {
            var totop = document.getElementById("totop-button")
            totop.style.display="none";

            totop.onclick = function () {
                window.scrollTo({
                    top: 0, behavior: "smooth"
                })
            }

            var pageHeight = 500;
            window.onscroll = function() {
                var backTop = document.documentElement.scrollTop || document.body.scrollTop;
                if (backTop > pageHeight) {
                    totop.style.display="block";
                } else {
                    totop.style.display="none";
                }
            }
        }
    </script>

    <style>
        body {
            background-image: url("imgs/background.jpg");
            background-repeat: no-repeat;
            background-position: right top;
            background-attachment: fixed;
            background-size: 100vmax;
        }
    </style>

</head>
<body>

    <div class="float-button">
        <button id="totop-button">
            &#9757
        </button>
    </div>

    <div class="wrapper">
        <div id="index-banner">
            <div id="index-banner-glass">
                <div class="container navbar">
                    <div class="navbar-frame frame-pull-left">
                        <img class="profile-picture alignable pull-right" src="./imgs/photo.jpg" />
                    </div>
                    <div class="navbar-content pull-left">
                        <a id="author-name" class="alignable pull-left author-name" href="">Jianyang Gu</a>
                        <br>
                        <p class="pull-left"><b>5th-Year Ph.D. Student @ Zhejiang University</b></p>
                        <br>
                        <a class="pull-left">College of Control Science and Engineering</a>
                        <br>
                        <p class="pull-left">Email: gu_jianyang@zju.edu.cn</p>
                    </div>
                </div>

                <ul id="navlist" class="navbar-ul">
                     <li class="alignable nav-list"><a href="mailto:gu_jianyang@zju.edu.cn">Mail</a>
                     /
                     </li>
                     <li class="alignable nav-list"><a href="https://github.com/vimar-gu">Github</a>
                     /
                     </li>
                     <li class="alignable nav-list"><a href="https://scholar.google.com/citations?user=8ZXbT18AAAAJ&hl=en">Google Scholar</a>
                     /
                     </li>
                     <li class="alignable nav-list"><a href="./vitae.pdf">Vitae</a>
                     </li>
                </ul>
            </div>
        </div>

        <div style="clear:both"></div>

        <div class="content">
            <div class="container main">
                <h2>About Me</h2>
                <p>
                    I'm currently a final-year Ph.D. student at 
                    <a href="http://cse.zju.edu.cn">College of Control Science and Engineering</a>, 
                    <a href="http://www.zju.edu.cn">Zhejiang University</a>, Hangzhou, China. 
                    I'm supervised by 
                    <a href="https://person.zju.edu.cn/en/jiangwei">Prof. Wei Jiang</a>.
                    Previously, I received my B.Eng. degree in Zhejiang University in 2019.
                    During Sep. 2022 - Oct. 2023, I am a visiting scholar at <a href="https://ai.comp.nus.edu.sg/">HPC AI Lab</a>,
                    <a href="https://www.nus.edu.sg/">National University of Singapore</a>,
                    under the supervision of <a href="https://www.comp.nus.edu.sg/~youy/">Prof. Yang You</a>.
                </p>
                <p>
                    <u>
                        I'm actively seeking postdoctoral opportunities and anticipate graduating in 2024. Please do not hesitate to contact me!
                    </u>
                </p>
                <div id="interests">
                    <h2>Research Interest</h2>
                    <ul>
                        <li>Data-centric Efficient Training</li>
                        <li>Unsupervised Object Re-Identification System</li>
                        <li>Computer Vision</li>
                    </ul>
                </div>
                <div id="publications" class="publications">
                    <h2>Selected Publications</h2>
                    <p>A full publication list is available in my <a href="https://scholar.google.com/citations?user=8ZXbT18AAAAJ&hl=en">google scholar</a> page.
                    <br>(* denotes equal contribution)</p>
                    <div class='year-block'>Data-centric Efficient Training</div>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/robust.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[arXiv] Group Distributionally Robust Dataset Distillation with Risk Minimization
                            <a class="paper-url" href="https://arxiv.org/pdf/2402.04676.pdf">[PDF]</a>
                            <p class="paper-content">Saeed Vahidian*, Mingyu Wang*, <b><u>Jianyang Gu*</u></b>, Vyacheslav Kungurtsev, Wei Jiang and Yiran Chen.</p>
                            <ul class="paper-description">
                                <li>Robust Dataset Distillation combines clustering with the minimization of a risk measure on the loss.</li>
                                <li>Robust Dataset Distillation substantially enhances the robustness across domain-shifted testing sets.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/minimaxdiffusion.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[arXiv] Efficient Dataset Distillation via Minimax Diffusion
                            <a class="paper-url" href="https://arxiv.org/pdf/2311.15529">[PDF]</a>
                            <a class="paper-url" href="https://github.com/vimar-gu/MinimaxDiffusion">[Code]</a></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Saeed Vahidian, Vyacheslav Kungurtsev, Haonan Wang, Wei Jiang, Yang You and Yiran Chen.</p>
                            <ul class="paper-description">
                                <li>Minimax Diffusion only requires 1 hour to finish the distillation for a 10-class subset of full-sized ImageNet.</li>
                                <li>Minimax Diffusion generates representative and diverse images, yielding state-of-the-art validation performance.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/ssd.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[AAAI 2024] Summarizing Stream Data for Memory-Restricted Online Continual Learning
                            <a class="paper-url" href="https://arxiv.org/pdf/2305.16645">[PDF]</a>
                            <a class="paper-url" href="https://github.com/vimar-gu/SSD">[Code]</a></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Kai Wang, Wei Jiang, and Yang You.</p>
                            <ul class="paper-description">
                                <li>SSD summarizes the informtion flow in online continual learning into informative samples.</li>
                                <li>SSD significantly improves the replay effects, especially for circumstances with restricted memory.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/dq.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[ICCV 2023] Dataset Quantization
                            <a class="paper-url" href="https://arxiv.org/abs/2308.10524">[PDF]</a>
                            <a class="paper-url" href="https://github.com/magic-research/Dataset_Quantization">[Code]</a></p>
                            <p class="paper-content">Daquan Zhou*, Kai Wang*, <b><u>Jianyang Gu*</u></b>, Dongze Lian, Xiangyu Peng, Yifan Zhang, Yang You, and Jiashi Feng.</p>
                            <ul class="paper-description">
                                <li>DQ targets at compressing large-scale datasets into subsets with lossless validation performance.</li>
                                <li>DQ only requires 60% ImageNet and 20% Alpaca's instruction tuning data to achieve lossless training.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/dream.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[ICCV 2023] DREAM: Efficient Dataset Distillation by Representative Matching
                            <a class="paper-url" href="https://arxiv.org/pdf/2302.14416">[PDF]</a>
                            <a class="paper-url" href="https://github.com/lyq312318224/DREAM">[Code]</a></p>
                            <p class="paper-content">Yanqing Liu*, <b><u>Jianyang Gu*</u></b>, Kai Wang, Zheng Zhu, Wei Jiang, and Yang You.</p>
                            <ul class="paper-description">
                                <li>DREAM aims to improve the training efficiency by only calculating matching metrics with representative samples.</li>
                                <li>DREAM only requires less than 20% of original iterations to achieve baseline performance.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/dim.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[arXiv] DiM: Distilling Dataset into Generative Model
                            <a class="paper-url" href="https://arxiv.org/pdf/2303.04707">[PDF]</a>
                            <a class="paper-url" href="https://github.com/vimar-gu/DiM">[Code]</a></p>
                            <p class="paper-content">Kai Wang*, <b><u>Jianyang Gu*</u></b>, Daquan Zhou, Zheng Zhu, Wei Jiang, and Yang You.</p>
                            <ul class="paper-description">
                                <li>DiM opens up a new generative scheme for dataset distillation by condensing the information into models.</li>
                                <li>DiM generates diverse images on the fly, providing outstanding distillation performance with small extra training consumption.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <ul>
                        <li>
                            <p class="paper-head">[ICLR 2024] InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning.
                            <a class="paper-url" href="https://arxiv.org/pdf/2303.04947">[PDF]</a>
                            <a class="paper-url" href="https://github.com/NUS-HPC-AI-Lab/InfoBatch">[Code]</a></p>
                            <p class="paper-content">Ziheng Qin*, Kai Wang*, Zangwei Zheng, <b><u>Jianyang Gu</u></b>, Xiangyu Peng, Zhaopan Xu, Daquan Zhou, Lei Shang, Baigui Sun, Xuansong Xie, Yang You</p>
                        </li>
                    </ul>
                    <div class='year-block'>Unsupervised Object Re-identification System</div>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/colorprompt.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[arXiv] Color Prompting for Data-Free Continual Unsupervised Domain Adaptive Person Re-Identification
                            <a class="paper-url" href="https://arxiv.org/abs/2308.10716">[PDF]</a>
                            <a class="paper-url" href="https://github.com/vimar-gu/ColorPromptReID">[Code]</a></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Hao Luo, Kai Wang, Wei Jiang, Yang You, and Jian Zhao.</p>
                            <ul class="paper-description">
                                <li>CoP learns the color distribution conditioned on the tasks and image contents during the continual learning process.</li>
                                <li>CoP provides substantial improvements on anti-forgetting and generalization without storing images of previous tasks.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/msinet.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[CVPR 2023] MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID
                            <a class="paper-url" href="https://arxiv.org/pdf/2303.07065">[PDF]</a>
                            <a class="paper-url" href="https://github.com/vimar-gu/MSINet">[Code]</a></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Kai Wang, Hao Luo, Chen Chen, Wei Jiang, Yuqiang Fang, Shanghang Zhang, Yang You, and Jian Zhao.</p>
                            <ul class="paper-description">
                                <li>MSINet searches the optimal interation between multi-scale branches with a twins contrastive mechanism.</li>
                                <li>MSINet yeilds better performance on both supervised and unsupervised tasks with limited parameter size.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/met.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[IEEE TIFS] Multi-View Evolutionary Training for Unsupervised Domain Adaptive Person Re-Identification
                            <a class="paper-url" href="https://ieeexplore.ieee.org/abstract/document/9672094/">[PDF]</a></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Weihua Chen, Hao Luo, Fan Wang, Hao Li, Wei Jiang, and Weijie Mao.</p>
                            <ul class="paper-description">
                                <li>MET effectively reduces the clustering noise from the dimensions of snapshot quality and temporal consistency.</li>
                                <li>MET significantly improves the training robustness by high-quality clustering results, leading to better model performance.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <ul>
                        <li>
                            <p class="paper-head">[IEEE TITS] Transformer-Based Domain-Specific Representation for Unsupervised Domain Adaptive Vehicle Re-Identification.
                            <a class="paper-url" href="https://ieeexplore.ieee.org/abstract/document/9967437">[PDF]</a></p>
                            <p class="paper-content">Ran Wei, <b><u>Jianyang Gu</u></b>, Shuting He, and Wei Jiang</p>
                        </li>
                        <li>
                            <p class="paper-head">[Pattern Anal Applic] An efficient global representation constrained by Angular Triplet loss for vehicle re-identification.
                            <a class="paper-url" href="https://link.springer.com/article/10.1007/s10044-020-00900-w">[PDF]</a></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Wei Jiang, Hao Luo, and Hongyan Yu.</p>
                        </li>
                        <li>
                            <p class="paper-head">[ECCVW 2020] 1st Place Solution to VisDA-2020: Bias Elimination for Domain Adaptive Pedestrian Re-identification.
                            <a class="paper-url" href="https://arxiv.org/pdf/2012.13498">[PDF]</a> <a class="paper-url" href="https://github.com/vimar-gu/Bias-Eliminate-DA-ReID">[Code]</a></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Hao Luo, Weihua Chen, Yiqi Jiang, Yuqi Zhang, Shuting He, Fan Wang, Hao Li, and Wei Jiang.</p>
                        </li>
                    </ul>
                </div>
                <div id="experiences">
                    <h2>Experience</h2>
                    <ul>
                        <li>
                            <p class="exp-head">OPPO Research Intern</p>
                            <p class="exp-content"><i>Nov. 2021 - Jun. 2022. </i>Focused on domain generalizable person re-identification and video action recognition.</p>
                        </li>
                        <li>
                            <p class="exp-head">Alibaba Research Intern</p>
                            <p class="exp-content"><i>Jun. 2020 - Apr. 2021. </i>Focused on unsupervised domain adaptive person re-identification.
                            Awarded as the annual outstanding research intern in 2020.</p>
                        </li>
                        <li>
                            <p class="exp-head">Yitu Tech. CI Intern</p>
                            <p class="exp-content"><i>May. 2018 - Sept. 2018. </i>Helped build up the continuous integration pipeline of products.</p>
                        </li>
                    </ul>
                </div>
                <div id="awards">
                    <h2>Competitions & Awards</h2>
                    <ul>
                        <li>
                            <p class="exp-head">ActivityNet Temporal Action Localization Challenge</p>
                            <p class="exp-content"><i>Third Place. </i>CVPR Workshop 2022.</p>
                        </li>
                        <li>
                            <p class="exp-head">SoccerNet Challenge</p>
                            <p class="exp-content"><i>Third Place. </i>CVPR Workshop 2022.</p>
                        </li>
                        <li>
                            <p class="exp-head">AICity Challenge</p>
                            <p class="exp-content"><i>First Place. </i>CVPR Workshop 2021.</p>
                        </li>
                        <li>
                            <p class="exp-head">National AI Challenge</p>
                            <p class="exp-content"><i>Second Prize. </i>2020.</p>
                        </li>
                        <li>
                            <p class="exp-head">Visual Domain Adaptation Challenge</p>
                            <p class="exp-content"><i>First Place. </i>ECCV Workshop 2020.</p>
                        </li>
                        <li>
                            <p class="exp-head">Robocup Montreal</p>
                            <p class="exp-content"><i>First Place. </i>2018.</p>
                        </li>
                    </ul>
                </div>
                <div id="service">
                    <h2>Academic Service</h2>
                    <ul>
                        <li>
                            <p>Conference Reviewer: CVPR, ICCV, ECCV</p>
                        </li>
                        <li>
                            <p>Journal Reviewer: IEEE TIV</p>
                        </li>
                    </ul>
                </div>
                <div id="other-info">
                    <h2>Other Information</h2>
                    <ul>
                        <li>
                            <p><b>President</b>, Student AI Association of Zhejiang University, Aug. 2020 - Jun. 2021</p>
                        </li>
                        <li>
                            <p>Part of my photography <a href="https://unsplash.com/@ypbehere">works</a>.</p>
                        </li>
                    </ul>
                </div>
                <div id="contact">
                    <h2>Contact Me</h2>
                    <p>You are welcome to contact me via Email or WeChat (ID: ypneverland)</p>
                </div>
            </div>
        </div>

        <div id="tail">
            <div id="tail-glass">
                Written by Jianyang Gu
            </div>
        </div>
    </div>
    <div style=" height:25px " ></div>
</body>
<footer>
</footer>
