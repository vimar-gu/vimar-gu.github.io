<!DOCTYPE html>
<html lang="en">
<head>

    <!-- Global site tag (gtag.js) - Google Analytics -->

    <title>Jianyang Gu's Page</title>

    <meta charset="utf-8">
    <!-- <link href='https://fonts.googleapis.com/css?family=Inconsolata:400,700' rel='stylesheet' type='text/css'> -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="viewport" content="initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <link href="./css/index-mobile.css" rel="stylesheet" type="text/css" media="screen and (max-width: 1000px)">
    <link href="./css/index.css" rel="stylesheet" type="text/css" media="screen and (min-width: 1000px)">

    <script type="text/javascript">
        window.onload = function () {
            var totop = document.getElementById("totop-button")
            totop.style.display="none";

            totop.onclick = function () {
                window.scrollTo({
                    top: 0, behavior: "smooth"
                })
            }

            var pageHeight = 500;
            window.onscroll = function() {
                var backTop = document.documentElement.scrollTop || document.body.scrollTop;
                if (backTop > pageHeight) {
                    totop.style.display="block";
                } else {
                    totop.style.display="none";
                }
            }
        }
    </script>

    <style>
        body {
            background-image: url("imgs/background.jpg");
            background-repeat: no-repeat;
            background-position: left top;
            background-attachment: fixed;
            background-size: 100vmax;
        }
    </style>

</head>
<body>

    <div class="float-button">
        <button id="totop-button">
            &#9757
        </button>
    </div>

    <div class="wrapper">
        <div id="index-banner">
            <div id="index-banner-glass">
                <div class="container navbar">
                    <div class="navbar-frame frame-pull-left">
                        <img class="profile-picture alignable pull-right" src="./imgs/photo.jpg" />
                    </div>
                    <div class="navbar-content pull-left">
                        <a id="author-name" class="alignable pull-left author-name" href="">Jianyang Gu</a>
                        <br>
                        <p class="pull-left"><b>Postdoc @ Ohio State University</b></p>
                        <br>
                        <a class="pull-left">Dept. of Computer Science and Engineering</a>
                        <br>
                        <p class="pull-left">Email: gu.1220[at]osu.edu</p>
                    </div>
                </div>

                <ul id="navlist" class="navbar-ul">
                     <li class="alignable nav-list"><a href="mailto:gu.1220@osu.edu">Mail</a>
                     /
                     </li>
                     <li class="alignable nav-list"><a href="https://github.com/vimar-gu">Github</a>
                     /
                     </li>
                     <li class="alignable nav-list"><a href="https://scholar.google.com/citations?user=8ZXbT18AAAAJ&hl=en">Google Scholar</a>
                     /
                     </li>
                     <li class="alignable nav-list"><a href="./vitae.pdf">Vitae</a>
                     </li>
                </ul>
            </div>
        </div>

        <div style="clear:both"></div>

        <div class="content">
            <div class="container main">
                <h2>About Me</h2>
                <p>
                    I am currently a postdoctoral researcher at the
                    <a href="https://cse.osu.edu/">Dept. of Computer Science and Engineering</a>,
                    the Ohio State University.
                    I'm fortunate to work with
                    <a href="https://sites.google.com/view/wei-lun-harry-chao/home">Wei-Lun Chao</a>,
                    <a href="https://ysu1989.github.io">Yu Su</a> and
                    <a href="https://cse.osu.edu/people/berger-wolf.1">Tanya Berger-Wolf</a>.
                    I obtained my Ph.D. in 2024 from 
                    <a href="http://cse.zju.edu.cn">College of Control Science and Engineering</a>, 
                    <a href="http://www.zju.edu.cn">Zhejiang University</a>, Hangzhou, China. 
                    I was supervised by 
                    <a href="https://person.zju.edu.cn/en/jiangwei">Prof. Wei Jiang</a>.
                    Previously, I received my B.Eng. in Zhejiang University in 2019.
                    During Sep. 2022 - Oct. 2023, I was a visiting scholar at <a href="https://ai.comp.nus.edu.sg/">HPC AI Lab</a>,
                    <a href="https://www.nus.edu.sg/">National University of Singapore</a>,
                    under the supervision of <a href="https://www.comp.nus.edu.sg/~youy/">Prof. Yang You</a>.
                </p>
                <div id="interests">
                    <h2>Research Interest</h2>
                    <ul>
                        <li>Interpretable and Explainable AI for Science</li>
                        <li>Data-centric Efficient Training</li>
                        <li>Computer Vision</li>
                    </ul>
                </div>
                <div id="publications" class="publications">
                    <h2>Selected Publications</h2>
                    <p>A full publication list is available in my <a href="https://scholar.google.com/citations?user=8ZXbT18AAAAJ&hl=en">google scholar</a> page.
                    <br>(* denotes equal contribution)</p>
                    <div class='year-block'>Interpretable and Explainable AI for Science</div>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/biocap.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[arXiv] BioCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models
                            <a class="paper-url" href="https://arxiv.org/pdf/2510.20095">[PDF]</a>
                            <a class="paper-url" href="https://github.com/Imageomics/biocap">[Code]</a>
                            <a class="paper-url" href="https://imageomics.github.io/biocap/">[Webpage]</a></p>
                            <p class="paper-content">Ziheng Zhang, Xinyue Ma, Arpita Chowdhury, Elizabeth G Campolongo, Matthew J Thompson, Net Zhang, Samuel Stevens, Hilmar Lapp, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao, and <b><u>Jianyang Gu</u></b>.</p>
                            <ul class="paper-description">
                                <li>BioCAP integrates instance-level synthetic captions beyond species names in multimodal alignment.</li>
                                <li>BioCAP demonstrates rich understanding of biological semantics.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/bioclip.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable"><a>[NeurIPS 2025 Spotlight]</a> BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning
                            <a class="paper-url" href="https://arxiv.org/pdf/2505.23883">[PDF]</a>
                            <a class="paper-url" href="https://github.com/Imageomics/bioclip-2">[Code]</a>
                            <a class="paper-url" href="https://imageomics.github.io/bioclip-2/">[Webpage]</a></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Samuel Stevens, Elizabeth G Campolongo, Matthew J Thompson, Net Zhang, Jiaman Wu, Andrei Kopanev, Zheda Mai, Alexander E White, James Balhoff, Wasila Dahdul, Daniel Rubenstein, Hilmar Lapp, Tanya Berger-Wolf, Wei-Lun Chao, and Yu Su.</p>
                            <ul class="paper-description">
                                <li>BioCLIP 2, trained on TreeOfLife-200M, demonstrates state-of-the-art species classification performance.</li>
                                <li>BioCLIP 2 shows emergent properties of inter-species ecological alignment and intra-species variation separation.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/finercam.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[CVPR 2025] Finer-CAM: Spotting the Difference Reveals Finer Details for Visual Explanation
                            <a class="paper-url" href="https://arxiv.org/pdf/2501.11309">[PDF]</a>
                            <a class="paper-url" href="https://github.com/Imageomics/Finer-CAM">[Code]</a>
                            <a class="paper-url" href="https://colab.research.google.com/drive/1plLrL7vszVD5r71RGX3YOEXEBmITkT90?usp=sharing">[Demo]</a></p>
                            <p class="paper-content">Ziheng Zhang*, <b><u>Jianyang Gu*</u></b>, Arpita Chowdhury, Zheda Mai, David Carlyn, Tanya Berger-Wolf, Yu Su, and Wei-Lun Chao.</p>
                            <ul class="paper-description">
                                <li>Finer-CAM compares the target class with similar classes for more fine-grained and discriminative explanation.</li>
                                <li>Finer-CAM can be extended to multi-modal scenarios to provide accurate localization of text concepts.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <ul>
                        <li>
                            <p class="paper-head">[CVPR 2025] Prompt-CAM: Making Vision Transformers Interpretable for Fine-Grained Analysis.
                            <a class="paper-url" href="https://arxiv.org/pdf/2501.09333">[PDF]</a>
                            <a class="paper-url" href="https://github.com/Imageomics/Prompt_CAM">[Code]</a></p>
                            <p class="paper-content">Arpita Chowdhury, Dipanjyoti Paul, Zheda Mai, <b><u>Jianyang Gu</u></b>, Ziheng Zhang, Kazi Sajeed Mehrab, Elizabeth G Campolongo, Daniel Rubenstein, Charles V Stewart, Anuj Karpatne, Tanya Berger-Wolf, Yu Su, and Wei-Lun Chao.</p>
                        </li>
                        <li>
                            <p class="paper-head">[CVPR Workshop] Static Segmentation by Tracking: A Frustratingly Label-Efficient Approach to Fine-Grained Segmentation.
                            <a class="paper-url" href="https://arxiv.org/pdf/2501.06749">[PDF]</a>
                            <a class="paper-url" href="https://github.com/Imageomics/SST">[Code]</a></p>
                            <p class="paper-content">Zhenyang Feng, Zihe Wang, <b><u>Jianyang Gu</u></b>, ..., Tanya Berger-Wolf, Yu Su, and Wei-Lun Chao.</p>
                        </li>
                    </ul>
                    <div class='year-block'>Data-centric Efficient Training</div>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/minimaxdiffusion.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[CVPR 2024] Efficient Dataset Distillation via Minimax Diffusion
                            <a class="paper-url" href="https://arxiv.org/pdf/2311.15529">[PDF]</a>
                            <a class="paper-url" href="https://github.com/vimar-gu/MinimaxDiffusion">[Code]</a></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Saeed Vahidian, Vyacheslav Kungurtsev, Haonan Wang, Wei Jiang, Yang You and Yiran Chen.</p>
                            <ul class="paper-description">
                                <li>Minimax Diffusion only requires 1 hour to finish the distillation for a 10-class subset of full-sized ImageNet.</li>
                                <li>Minimax Diffusion generates representative and diverse images, yielding state-of-the-art validation performance.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/ssd.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[AAAI 2024] Summarizing Stream Data for Memory-Restricted Online Continual Learning
                            <a class="paper-url" href="https://arxiv.org/pdf/2305.16645">[PDF]</a>
                            <a class="paper-url" href="https://github.com/vimar-gu/SSD">[Code]</a></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Kai Wang, Wei Jiang, and Yang You.</p>
                            <ul class="paper-description">
                                <li>SSD summarizes the informtion flow in online continual learning into informative samples.</li>
                                <li>SSD significantly improves the replay effects, especially for circumstances with restricted memory.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/dream.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[ICCV 2023] DREAM: Efficient Dataset Distillation by Representative Matching
                            <a class="paper-url" href="https://arxiv.org/pdf/2302.14416">[PDF]</a>
                            <a class="paper-url" href="https://github.com/lyq312318224/DREAM">[Code]</a></p>
                            <p class="paper-content">Yanqing Liu*, <b><u>Jianyang Gu*</u></b>, Kai Wang, Zheng Zhu, Wei Jiang, and Yang You.</p>
                            <ul class="paper-description">
                                <li>DREAM aims to improve the training efficiency by only calculating matching metrics with representative samples.</li>
                                <li>DREAM only requires less than 20% of original iterations to achieve baseline performance.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <ul>
                        <li>
                            <p class="paper-head">[ICML 2025] Taming Diffusion for Dataset Distillation with High Representativeness.
                            <a class="paper-url" href="https://arxiv.org/pdf/2505.18399">[PDF]</a>
                            <a class="paper-url" href="https://github.com/lin-zhao-resoLve/D3HR">[Code]</a></p>
                            <p class="paper-content">Lin Zhao, Yushu Wu, Xinru Jiang, <b><u>Jianyang Gu</u></b>, Yanzhi Wang, Xiaolin Xu, Pu Zhao, and Xue Lin.</p>
                        </li>
                        <li>
                            <p class="paper-head">[ICLR 2025] Group Distributionally Robust Dataset Distillation with Risk Minimization.
                            <a class="paper-url" href="https://arxiv.org/pdf/2402.04676.pdf">[PDF]</a>
                            <a class="paper-url" href="https://github.com/Mming11/RobustDatasetDistillation">[Code]</a></p>
                            <p class="paper-content">Saeed Vahidian*, Mingyu Wang*, <b><u>Jianyang Gu*</u></b>, Vyacheslav Kungurtsev, Wei Jiang and Yiran Chen.</p>
                        </li>
                        <li>
                            <p class="paper-head"><a>[ICLR 2024 Oral]</a> InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning.
                            <a class="paper-url" href="https://arxiv.org/pdf/2303.04947">[PDF]</a>
                            <a class="paper-url" href="https://github.com/NUS-HPC-AI-Lab/InfoBatch">[Code]</a></p>
                            <p class="paper-content">Ziheng Qin*, Kai Wang*, Zangwei Zheng, <b><u>Jianyang Gu</u></b>, Xiangyu Peng, Zhaopan Xu, Daquan Zhou, Lei Shang, Baigui Sun, Xuansong Xie, Yang You.</p>
                        </li>
                        <li>
                            <p class="paper-head">[ICCV 2023] Dataset Quantization.
                            <a class="paper-url" href="https://arxiv.org/abs/2308.10524">[PDF]</a>
                            <a class="paper-url" href="https://github.com/magic-research/Dataset_Quantization">[Code]</a></p>
                            <p class="paper-content">Daquan Zhou*, Kai Wang*, <b><u>Jianyang Gu*</u></b>, Dongze Lian, Xiangyu Peng, Yifan Zhang, Yang You, and Jiashi Feng.</p>
                        </li>
                    </ul>
                    <div class='year-block'>Unsupervised Object Re-identification System</div>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/colorprompt.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[arXiv] Color Prompting for Data-Free Continual Unsupervised Domain Adaptive Person Re-Identification
                            <a class="paper-url" href="https://arxiv.org/abs/2308.10716">[PDF]</a>
                            <a class="paper-url" href="https://github.com/vimar-gu/ColorPromptReID">[Code]</a></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Hao Luo, Kai Wang, Wei Jiang, Yang You, and Jian Zhao.</p>
                            <ul class="paper-description">
                                <li>CoP learns the color distribution conditioned on the tasks and image contents during the continual learning process.</li>
                                <li>CoP provides substantial improvements on anti-forgetting and generalization without storing images of previous tasks.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/msinet.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[CVPR 2023] MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID
                            <a class="paper-url" href="https://arxiv.org/pdf/2303.07065">[PDF]</a>
                            <a class="paper-url" href="https://github.com/vimar-gu/MSINet">[Code]</a></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Kai Wang, Hao Luo, Chen Chen, Wei Jiang, Yuqiang Fang, Shanghang Zhang, Yang You, and Jian Zhao.</p>
                            <ul class="paper-description">
                                <li>MSINet searches the optimal interation between multi-scale branches with a twins contrastive mechanism.</li>
                                <li>MSINet yeilds better performance on both supervised and unsupervised tasks with limited parameter size.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/met.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable">[IEEE TIFS] Multi-View Evolutionary Training for Unsupervised Domain Adaptive Person Re-Identification
                            <a class="paper-url" href="https://ieeexplore.ieee.org/abstract/document/9672094/">[PDF]</a></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Weihua Chen, Hao Luo, Fan Wang, Hao Li, Wei Jiang, and Weijie Mao.</p>
                            <ul class="paper-description">
                                <li>MET effectively reduces the clustering noise from the dimensions of snapshot quality and temporal consistency.</li>
                                <li>MET significantly improves the training robustness by high-quality clustering results, leading to better model performance.</li>
                            </ul>
                        </div>
                    </div>
                    <hr>
                    <ul>
                        <li>
                            <p class="paper-head">[IEEE TITS] Transformer-Based Domain-Specific Representation for Unsupervised Domain Adaptive Vehicle Re-Identification.
                            <a class="paper-url" href="https://ieeexplore.ieee.org/abstract/document/9967437">[PDF]</a></p>
                            <p class="paper-content">Ran Wei, <b><u>Jianyang Gu</u></b>, Shuting He, and Wei Jiang.</p>
                        </li>
                        <li>
                            <p class="paper-head">[Pattern Anal Applic] An efficient global representation constrained by Angular Triplet loss for vehicle re-identification.
                            <a class="paper-url" href="https://link.springer.com/article/10.1007/s10044-020-00900-w">[PDF]</a></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Wei Jiang, Hao Luo, and Hongyan Yu.</p>
                        </li>
                        <li>
                            <p class="paper-head">[ECCVW 2020] 1st Place Solution to VisDA-2020: Bias Elimination for Domain Adaptive Pedestrian Re-identification.
                            <a class="paper-url" href="https://arxiv.org/pdf/2012.13498">[PDF]</a> <a class="paper-url" href="https://github.com/vimar-gu/Bias-Eliminate-DA-ReID">[Code]</a></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Hao Luo, Weihua Chen, Yiqi Jiang, Yuqi Zhang, Shuting He, Fan Wang, Hao Li, and Wei Jiang.</p>
                        </li>
                    </ul>
                </div>
                <div id="experiences">
                    <h2>Experience</h2>
                    <ul>
                        <li>
                            <p class="exp-head">OPPO Research Intern</p>
                            <p class="exp-content"><i>Nov. 2021 - Jun. 2022. </i>Focused on domain generalizable person re-identification and video action recognition.</p>
                        </li>
                        <li>
                            <p class="exp-head">Alibaba Research Intern</p>
                            <p class="exp-content"><i>Jun. 2020 - Apr. 2021. </i>Focused on unsupervised domain adaptive person re-identification.
                            Awarded as the annual outstanding research intern in 2020.</p>
                        </li>
                        <li>
                            <p class="exp-head">Yitu Tech. CI Intern</p>
                            <p class="exp-content"><i>May. 2018 - Sept. 2018. </i>Helped build up the continuous integration pipeline of products.</p>
                        </li>
                    </ul>
                </div>
                <div id="awards">
                    <h2>Competitions & Awards</h2>
                    <ul>
                        <li>
                            <p class="exp-head">NeurIPS Scholar Award</p>
                            <p class="exp-content">2025.</p>
                        </li>
                        <li>
                            <p class="exp-head">OSU CSE Research Staff Award</p>
                            <p class="exp-content">2025.</p>
                        </li>
                        <li>
                            <p class="exp-head">AAAI Scholarship</p>
                            <p class="exp-content">2024.</p>
                        </li>
                        <li>
                            <p class="exp-head">ActivityNet Temporal Action Localization Challenge</p>
                            <p class="exp-content"><i>Third Place. </i>CVPR Workshop 2022.</p>
                        </li>
                        <li>
                            <p class="exp-head">SoccerNet Challenge</p>
                            <p class="exp-content"><i>Third Place. </i>CVPR Workshop 2022.</p>
                        </li>
                        <li>
                            <p class="exp-head">AICity Challenge</p>
                            <p class="exp-content"><i>First Place. </i>CVPR Workshop 2021.</p>
                        </li>
                        <li>
                            <p class="exp-head">National AI Challenge</p>
                            <p class="exp-content"><i>Second Prize. </i>2020.</p>
                        </li>
                        <li>
                            <p class="exp-head">Visual Domain Adaptation Challenge</p>
                            <p class="exp-content"><i>First Place. </i>ECCV Workshop 2020.</p>
                        </li>
                        <li>
                            <p class="exp-head">Robocup Montreal</p>
                            <p class="exp-content"><i>First Place. </i>2018.</p>
                        </li>
                    </ul>
                </div>
                <div id="service">
                    <h2>Academic Service</h2>
                    <ul>
                        <li>
                            <p class="exp-head">Workshop Organization</p>
                            <p class="exp-content"><i>Lead organizer. </i><a class="paper-url" href="https://imageomics.github.io/Imageomics-NeurIPS-2025/">Third Workshop on Imageomics</a> @ NeurIPS2025</p>
                            <p class="exp-content"><i>Co-organizer. </i><a class="paper-url" href="https://sites.google.com/view/imageomics-aaai-25/">Second Workshop on Imageomics</a> @ AAAI2025</p>
                            <p class="exp-content"><i>Co-organizer. </i><a class="paper-url" href="https://hdr-ecosystem.github.io/hdr-ad-challenge-webpage/aaai-workshop2024.html">Anomaly Detection in Scientific Domains Workshop</a> @ AAAI2025</p>
                            <p class="exp-content"><i>PC Member. </i><a class="paper-url" href="https://sites.google.com/view/dd-cvpr2024/home">First Workshop on Dataset Distillation</a> @ CVPR2024 </p>
                        </li>
                        <li>
                            <p class="exp-head">Conference Area Chair</p>
                            <p class="exp-content">ICLR 2026</p>
                        </li>
                        <li>
                            <p class="exp-head">Conference Reviewer</p>
                            <p class="exp-content">CVPR, ICCV, ECCV, ICLR, ICML, NeurIPS, ACMMM, WACV, ACCV</p>
                        </li>
                        <li>
                            <p class="exp-head">Journal Reviewer</p>
                            <p class="exp-content">PNAS, IEEE TPAMI, PR, CVIU, IEEE TCSVT</p>
                        </li>
                    </ul>
                </div>
                <div id="other-info">
                    <h2>Other Information</h2>
                    <ul>
                        <li>
                            <p><b>President</b>, Student AI Association of Zhejiang University, Aug. 2020 - Jun. 2021</p>
                        </li>
                        <li>
                            <p>Part of my photography <a href="https://unsplash.com/@ypbehere">works</a>.</p>
                        </li>
                    </ul>
                </div>
                <div id="contact">
                    <h2>Contact Me</h2>
                    <p>You are welcome to contact me via Email.</p>
                </div>
            </div>
        </div>

        <div class="visitor">
            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=szeRYJgXin9srYykakESD1C25nuaavZ8st4jGt2CCQM&cl=ffffff&w=a"></script>
        </div>

        <div id="tail">
            <div id="tail-glass">
                Written by Jianyang Gu
            </div>
        </div>
    </div>
    <div style=" height:25px " ></div>
</body>
<footer>
</footer>
