<!DOCTYPE html>
<html lang="en">
<head>

    <!-- Global site tag (gtag.js) - Google Analytics -->

    <title>Jianyang Gu's Page</title>

    <meta charset="utf-8">
    <!-- <link href='https://fonts.googleapis.com/css?family=Inconsolata:400,700' rel='stylesheet' type='text/css'> -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="viewport" content="initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;500;600&display=swap" rel="stylesheet">
    <link href="./css/index-mobile.css" rel="stylesheet" type="text/css" media="screen and (max-width: 1000px)">
    <link href="./css/index.css" rel="stylesheet" type="text/css" media="screen and (min-width: 1000px)">

    <script type="text/javascript">
            window.onload = function () {
                var totop = document.getElementById("totop-button")
                totop.style.display="none";

            totop.onclick = function () {
                window.scrollTo({
                    top: 0, behavior: "smooth"
                })
            }

            var pageHeight = 500;
                window.onscroll = function() {
                    var backTop = document.documentElement.scrollTop || document.body.scrollTop;
                    if (backTop > pageHeight) {
                        totop.style.display="block";
                    } else {
                        totop.style.display="none";
                    }
                }

                function getLastAuthor(text) {
                    var cleaned = text.replace(/\s*\.$/, "");
                    var andIndex = cleaned.lastIndexOf(" and ");
                    if (andIndex !== -1) {
                        return cleaned.slice(andIndex + 5).trim();
                    }
                    var lastComma = cleaned.lastIndexOf(",");
                    if (lastComma !== -1) {
                        return cleaned.slice(lastComma + 1).trim();
                    }
                    return cleaned;
                }

                function buildAuthorPreview(text, name, alwaysInclude) {
                    var limit = 60;
                    if (text.length <= limit) {
                        return text;
                    }
                    var head = text.slice(0, limit);
                    var lastComma = head.lastIndexOf(",");
                    if (lastComma > 0) {
                        head = head.slice(0, lastComma);
                    }
                    if (alwaysInclude && name !== alwaysInclude) {
                        return head + ", \u2026 " + alwaysInclude + ", " + name + ".";
                    }
                    return head + ", \u2026 " + name + ".";
                }

                var authorItems = document.querySelectorAll(".paper-content");
                for (var i = 0; i < authorItems.length; i++) {
                    var item = authorItems[i];
                    var fullText = item.textContent.trim();
                    if (fullText.length < 80) {
                        continue;
                    }
                    var fullHtml = item.innerHTML;
                    var keepLast = item.getAttribute("data-keep-last") || getLastAuthor(fullText);
                    var previewText = buildAuthorPreview(fullText, keepLast);
                    var previewHtml = previewText;
                    if (fullHtml.indexOf("<b><u>Jianyang Gu") !== -1 && previewText.indexOf("Jianyang Gu") !== -1) {
                        previewHtml = previewText.replace(
                            "Jianyang Gu",
                            "<b><u>Jianyang Gu</u></b>"
                        );
                    }
                    var authorText = document.createElement("span");
                    authorText.className = "author-text";
                    authorText.dataset.preview = previewHtml;
                    authorText.dataset.full = fullHtml;
                    authorText.innerHTML = previewHtml;
                    item.innerHTML = "";
                    item.appendChild(authorText);
                    item.classList.add("has-preview");
                    item.classList.add("is-collapsed");
                    var btn = document.createElement("button");
                    btn.type = "button";
                    btn.className = "author-toggle";
                    btn.textContent = "Show more";
                    btn.onclick = function (target, textEl) {
                        return function () {
                            var expanded = target.classList.toggle("is-expanded");
                            target.classList.toggle("is-collapsed", !expanded);
                            if (target.classList.contains("has-preview")) {
                                textEl.innerHTML = expanded ? textEl.dataset.full : textEl.dataset.preview;
                            }
                            this.textContent = expanded ? "Show less" : "Show more";
                        };
                    }(item, authorText);
                    item.appendChild(btn);
                }
            }
    </script>

    <style>
        body {
            background-image: url("imgs/background.jpg");
            background-repeat: no-repeat;
            background-position: left top;
            background-attachment: fixed;
            background-size: 100vmax;
        }
    </style>

</head>
<body>

    <div class="float-button">
        <button id="totop-button">
            &#9757
        </button>
    </div>

    <div class="wrapper">
        <div id="index-banner">
            <div id="index-banner-glass">
                <div class="container navbar">
                    <div class="navbar-frame">
                        <img class="profile-picture alignable pull-right" src="./imgs/photo.jpg" />
                    </div>
                    <div class="navbar-content">
                        <a id="author-name" class="author-name" href="">Jianyang Gu</a>
                        <div class="hero-role">Postdoc @ The Ohio State University</div>
                        <div class="hero-meta">Dept. of Computer Science and Engineering</div>
                        <div class="hero-meta">Email: gu.1220[at]osu.edu</div>
                        <ul id="navlist" class="navbar-ul hero-actions">
                             <li class="nav-list"><a href="./vitae.pdf">CV</a></li>
                             <li class="nav-list"><a href="https://scholar.google.com/citations?user=8ZXbT18AAAAJ&hl=en">Scholar</a></li>
                             <li class="nav-list"><a href="https://github.com/vimar-gu">GitHub</a></li>
                             <li class="nav-list"><a href="mailto:gu.1220@osu.edu">Email</a></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <div style="clear:both"></div>

        <div class="content">
            <div class="container main">
                <h2>About Me</h2>
                <p>
                    I am currently a postdoctoral researcher at the
                    <a href="https://cse.osu.edu/">Dept. of Computer Science and Engineering</a>,
                    The Ohio State University.
                    I'm fortunate to work with
                    <a href="https://sites.google.com/view/wei-lun-harry-chao/home">Wei-Lun Chao</a>,
                    <a href="https://ysu1989.github.io">Yu Su</a> and
                    <a href="https://cse.osu.edu/people/berger-wolf.1">Tanya Berger-Wolf</a>.
                    I obtained my Ph.D. in 2024 from 
                    <a href="http://cse.zju.edu.cn">College of Control Science and Engineering</a>, 
                    <a href="http://www.zju.edu.cn">Zhejiang University</a>, Hangzhou, China. 
                    I was supervised by 
                    <a href="https://person.zju.edu.cn/en/jiangwei">Wei Jiang</a>.
                    Previously, I received my B.Eng. in Zhejiang University in 2019.
                    During Sep. 2022 - Oct. 2023, I was a visiting scholar at <a href="https://ai.comp.nus.edu.sg/">HPC AI Lab</a>,
                    <a href="https://www.nus.edu.sg/">National University of Singapore</a>,
                    under the supervision of <a href="https://www.comp.nus.edu.sg/~youy/">Yang You</a>.
                </p>
                <div id="interests">
                    <h2>Research Interest</h2>
                    <ul>
                        <li>Interpretable and Explainable AI for Science</li>
                        <li>Data-centric Efficient Training</li>
                        <li>Computer Vision</li>
                    </ul>
                </div>
                <div id="publications" class="publications">
                    <h2>Selected Publications</h2>
                    <p>A full publication list is available in my <a href="https://scholar.google.com/citations?user=8ZXbT18AAAAJ&hl=en">google scholar</a> page.
                    <br>(* denotes equal contribution)</p>
                    <div class='year-block'>Interpretable and Explainable AI for Science</div>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/biocap.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable"><span class="paper-title">BioCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models</span></p>
                            <p class="paper-description">Integrate instance-level synthetic captions in multimodal alignment and demonstrate rich understanding of biological semantics.</p>
                            <p class="paper-content" data-keep-last="Jianyang Gu">Ziheng Zhang, Xinyue Ma, Arpita Chowdhury, Elizabeth G Campolongo, Matthew J Thompson, Net Zhang, Samuel Stevens, Hilmar Lapp, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao, and <b><u>Jianyang Gu</u></b>.</p>
                            
                            <span class="paper-meta"><span class="paper-badge">arXiv</span>
                             <a class="paper-url" href="https://arxiv.org/pdf/2510.20095">PDF</a>
                            <a class="paper-url" href="https://github.com/Imageomics/biocap">Code</a>
                            <a class="paper-url" href="https://imageomics.github.io/biocap/">Webpage</a></span>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/bioclip.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable"><span class="paper-title">BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning</span></p>
                            <p class="paper-description">Extend species classification training to emergent properties of inter-species ecological alignment and intra-species variation separation.</p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Samuel Stevens, Elizabeth G Campolongo, Matthew J Thompson, Net Zhang, Jiaman Wu, Andrei Kopanev, Zheda Mai, Alexander E White, James Balhoff, Wasila Dahdul, Daniel Rubenstein, Hilmar Lapp, Tanya Berger-Wolf, Wei-Lun Chao, and Yu Su.</p>
                            
                            <span class="paper-meta"><span class="paper-badge spotlight">NeurIPS 2025 Spotlight</span>
                             <a class="paper-url" href="https://arxiv.org/pdf/2505.23883">PDF</a>
                            <a class="paper-url" href="https://github.com/Imageomics/bioclip-2">Code</a>
                            <a class="paper-url" href="https://imageomics.github.io/bioclip-2/">Webpage</a></span>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/finercam.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable"><span class="paper-title">Finer-CAM: Spotting the Difference Reveals Finer Details for Visual Explanation</span></p>
                            <p class="paper-description">Compare the target class with similar classes for more fine-grained and discriminative explanation.</p>
                            <p class="paper-content">Ziheng Zhang*, <b><u>Jianyang Gu*</u></b>, Arpita Chowdhury, Zheda Mai, David Carlyn, Tanya Berger-Wolf, Yu Su, and Wei-Lun Chao.</p>
                            
                            <span class="paper-meta"><span class="paper-badge">CVPR 2025</span>
                             <a class="paper-url" href="https://arxiv.org/pdf/2501.11309">PDF</a>
                            <a class="paper-url" href="https://github.com/Imageomics/Finer-CAM">Code</a>
                            <a class="paper-url" href="https://colab.research.google.com/drive/1plLrL7vszVD5r71RGX3YOEXEBmITkT90?usp=sharing">Demo</a></span>
                        </div>
                    </div>
                    <hr>
                        <div class="card">
                            <p class="paper-head"><span class="paper-title">Prompt-CAM: Making Vision Transformers Interpretable for Fine-Grained Analysis.</span></p>
                            <p class="paper-content">Arpita Chowdhury, Dipanjyoti Paul, Zheda Mai, <b><u>Jianyang Gu</u></b>, Ziheng Zhang, Kazi Sajeed Mehrab, Elizabeth G Campolongo, Daniel Rubenstein, Charles V Stewart, Anuj Karpatne, Tanya Berger-Wolf, Yu Su, and Wei-Lun Chao.</p>
                            
                            <span class="paper-meta"><span class="paper-badge">CVPR 2025</span>
                             <a class="paper-url" href="https://arxiv.org/pdf/2501.09333">PDF</a>
                            <a class="paper-url" href="https://github.com/Imageomics/Prompt_CAM">Code</a></span>
                        </div>
                    <hr>

                        <div class="card">
                            <p class="paper-head"><span class="paper-title">Static Segmentation by Tracking: A Frustratingly Label-Efficient Approach to Fine-Grained Segmentation.</span></p>
                            <p class="paper-content">Zhenyang Feng, Zihe Wang, <b><u>Jianyang Gu</u></b>, Saul Ibaven Bueno, Tomasz Frelek, Advikaa Ramesh, Jingyan Bai, Lemeng Wang, Zanming Huang, Jinsu Yoo, Tai-Yu Pan, Arpita Chowdhury, Michelle Ramirez, Elizabeth G. Campolongo, Matthew J. Thompson, Christopher G. Lawrence, Sydne Record, Neil Rosser, Anuj Karpatne, Daniel Rubenstein, Hilmar Lapp, Charles V. Stewart, Tanya Berger-Wolf, Yu Su, and Wei-Lun Chao.</p>
                            
                            <span class="paper-meta"><span class="paper-badge oral">CVPR CV4Animals Workshop Oral</span>
                             <a class="paper-url" href="https://arxiv.org/pdf/2501.06749">PDF</a>
                            <a class="paper-url" href="https://github.com/Imageomics/SST">Code</a></span>
                        </div>
                    </ul>
                    <div class='year-block'>Data-centric Efficient Training</div>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/minimaxdiffusion.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable"><span class="paper-title">Efficient Dataset Distillation via Minimax Diffusion</span></p>
                            <p class="paper-description">Efficiently distill ImageNet and generate representative and diverse images, yielding state-of-the-art validation performance.</p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Saeed Vahidian, Vyacheslav Kungurtsev, Haonan Wang, Wei Jiang, Yang You and Yiran Chen.</p>
                            
                            <span class="paper-meta"><span class="paper-badge">CVPR 2024</span>
                             <a class="paper-url" href="https://arxiv.org/pdf/2311.15529">PDF</a>
                            <a class="paper-url" href="https://github.com/vimar-gu/MinimaxDiffusion">Code</a></span>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/ssd.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable"><span class="paper-title">Summarizing Stream Data for Memory-Restricted Online Continual Learning</span></p>
                            <p class="paper-description">Summarize the information flow in online continual learning into informative samples and significantly improve the replay effects under tight memory.</p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Kai Wang, Wei Jiang, and Yang You.</p>
                            
                            <span class="paper-meta"><span class="paper-badge">AAAI 2024</span>
                             <a class="paper-url" href="https://arxiv.org/pdf/2305.16645">PDF</a>
                            <a class="paper-url" href="https://github.com/vimar-gu/SSD">Code</a></span>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/dream.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable"><span class="paper-title">DREAM: Efficient Dataset Distillation by Representative Matching</span></p>
                            <p class="paper-description">Improve the training efficiency by only calculating matching metrics with representative samples.</p>
                            <p class="paper-content">Yanqing Liu*, <b><u>Jianyang Gu*</u></b>, Kai Wang, Zheng Zhu, Wei Jiang, and Yang You.</p>
                            
                            <span class="paper-meta"><span class="paper-badge">ICCV 2023</span>
                             <a class="paper-url" href="https://arxiv.org/pdf/2302.14416">PDF</a>
                            <a class="paper-url" href="https://github.com/lyq312318224/DREAM">Code</a></span>
                        </div>
                    </div>
                    <hr>
                        <div class="card">
                            <p class="paper-head"><span class="paper-title">Taming Diffusion for Dataset Distillation with High Representativeness.</span></p>
                            <p class="paper-content">Lin Zhao, Yushu Wu, Xinru Jiang, <b><u>Jianyang Gu</u></b>, Yanzhi Wang, Xiaolin Xu, Pu Zhao, and Xue Lin.</p>
                            
                            <span class="paper-meta"><span class="paper-badge">ICML 2025</span>
                             <a class="paper-url" href="https://arxiv.org/pdf/2505.18399">PDF</a>
                            <a class="paper-url" href="https://github.com/lin-zhao-resoLve/D3HR">Code</a></span>
                        </div>
                    <hr>
                        <div class="card">
                            <p class="paper-head"><span class="paper-title">Group Distributionally Robust Dataset Distillation with Risk Minimization.</span></p>
                            <p class="paper-content">Saeed Vahidian*, Mingyu Wang*, <b><u>Jianyang Gu*</u></b>, Vyacheslav Kungurtsev, Wei Jiang and Yiran Chen.</p>
                            
                            <span class="paper-meta"><span class="paper-badge">ICLR 2025</span>
                             <a class="paper-url" href="https://arxiv.org/pdf/2402.04676.pdf">PDF</a>
                            <a class="paper-url" href="https://github.com/Mming11/RobustDatasetDistillation">Code</a></span>
                        </div>
                    <hr>
                        <div class="card">
                            <p class="paper-head"><span class="paper-title">InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning.</span></p>
                            <p class="paper-content">Ziheng Qin*, Kai Wang*, Zangwei Zheng, <b><u>Jianyang Gu</u></b>, Xiangyu Peng, Zhaopan Xu, Daquan Zhou, Lei Shang, Baigui Sun, Xuansong Xie, Yang You.</p>
                            
                            <span class="paper-meta"><span class="paper-badge oral">ICLR 2024 Oral</span>
                             <a class="paper-url" href="https://arxiv.org/pdf/2303.04947">PDF</a>
                            <a class="paper-url" href="https://github.com/NUS-HPC-AI-Lab/InfoBatch">Code</a></span>
                        </div>
                    <hr>
                        <div class="card">
                            <p class="paper-head"><span class="paper-title">Dataset Quantization.</span></p>
                            <p class="paper-content">Daquan Zhou*, Kai Wang*, <b><u>Jianyang Gu*</u></b>, Dongze Lian, Xiangyu Peng, Yifan Zhang, Yang You, and Jiashi Feng.</p>
                            
                            <span class="paper-meta"><span class="paper-badge">ICCV 2023</span>
                             <a class="paper-url" href="https://arxiv.org/abs/2308.10524">PDF</a>
                            <a class="paper-url" href="https://github.com/magic-research/Dataset_Quantization">Code</a></span>
                        </div>
                    <div class='year-block'>Unsupervised Object Re-identification System</div>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/colorprompt.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable"><span class="paper-title">Color Prompting for Data-Free Continual Unsupervised Domain Adaptive Person Re-Identification</span></p>
                            <p class="paper-description">Learn the task-conditioned color distribution to reduce forgetting and enhance generalization without storing previous images.</p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Hao Luo, Kai Wang, Wei Jiang, Yang You, and Jian Zhao.</p>
                            
                            <span class="paper-meta"><span class="paper-badge">arXiv</span>
                             <a class="paper-url" href="https://arxiv.org/abs/2308.10716">PDF</a>
                            <a class="paper-url" href="https://github.com/vimar-gu/ColorPromptReID">Code</a></span>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/msinet.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable"><span class="paper-title">MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID</span></p>
                            <p class="paper-description">Search the optimal interaction between multi-scale branches and yield better supervised and unsupervised performance with limited parameter size.</p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Kai Wang, Hao Luo, Chen Chen, Wei Jiang, Yuqiang Fang, Shanghang Zhang, Yang You, and Jian Zhao.</p>
                            
                            <span class="paper-meta"><span class="paper-badge">CVPR 2023</span>
                             <a class="paper-url" href="https://arxiv.org/pdf/2303.07065">PDF</a>
                            <a class="paper-url" href="https://github.com/vimar-gu/MSINet">Code</a></span>
                        </div>
                    </div>
                    <hr>
                    <div class="card">
                        <div class="card-frame frame-pull-left">
                            <img class="card-picture alignable" src="./imgs/pubs/met.png" />
                        </div>
                        <div class="card-content pull-left">
                            <p class="card-title alignable"><span class="paper-title">Multi-View Evolutionary Training for Unsupervised Domain Adaptive Person Re-Identification</span></p>
                            <p class="paper-description">Effectively reduce the clustering noise from the dimensions of snapshot quality and temporal consistency in unsupervised domain adaptation.</p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Weihua Chen, Hao Luo, Fan Wang, Hao Li, Wei Jiang, and Weijie Mao.</p>
                            
                            <span class="paper-meta"><span class="paper-badge">IEEE TIFS</span>
                             <a class="paper-url" href="https://ieeexplore.ieee.org/abstract/document/9672094/">PDF</a></span>
                        </div>
                    </div>
                    <hr>
                        <div class="card">
                            <p class="paper-head"><span class="paper-title">Transformer-Based Domain-Specific Representation for Unsupervised Domain Adaptive Vehicle Re-Identification.</span></p>
                            <p class="paper-content">Ran Wei, <b><u>Jianyang Gu</u></b>, Shuting He, and Wei Jiang.</p>
                            
                            <span class="paper-meta"><span class="paper-badge">IEEE TITS</span>
                             <a class="paper-url" href="https://ieeexplore.ieee.org/abstract/document/9967437">PDF</a></span>
                        </div>
                    <hr>
                        <div class="card">
                            <p class="paper-head"><span class="paper-title">An efficient global representation constrained by Angular Triplet loss for vehicle re-identification.</span></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Wei Jiang, Hao Luo, and Hongyan Yu.</p>
                            
                            <span class="paper-meta"><span class="paper-badge">Pattern Anal Applic</span>
                             <a class="paper-url" href="https://link.springer.com/article/10.1007/s10044-020-00900-w">PDF</a></span>
                        </div>
                    <hr>
                        <div class="card">
                            <p class="paper-head"><span class="paper-title">1st Place Solution to VisDA-2020: Bias Elimination for Domain Adaptive Pedestrian Re-identification.</span></p>
                            <p class="paper-content"><b><u>Jianyang Gu</u></b>, Hao Luo, Weihua Chen, Yiqi Jiang, Yuqi Zhang, Shuting He, Fan Wang, Hao Li, and Wei Jiang.</p>
                            
                            <span class="paper-meta"><span class="paper-badge">ECCVW 2020</span>
                             <a class="paper-url" href="https://arxiv.org/pdf/2012.13498">PDF</a> <a class="paper-url" href="https://github.com/vimar-gu/Bias-Eliminate-DA-ReID">Code</a></span>
                        </div>
                </div>
                <div id="experiences">
                    <h2>Experience</h2>
                    <ul>
                        <li>
                            <p class="exp-head">OPPO Research Intern</p>
                            <p class="exp-content"><i>Nov. 2021 - Jun. 2022. </i>Focused on domain generalizable person re-identification and video action recognition.</p>
                        </li>
                        <li>
                            <p class="exp-head">Alibaba Research Intern</p>
                            <p class="exp-content"><i>Jun. 2020 - Apr. 2021. </i>Focused on unsupervised domain adaptive person re-identification.
                            Awarded as the annual outstanding research intern in 2020.</p>
                        </li>
                        <li>
                            <p class="exp-head">Yitu Tech. CI Intern</p>
                            <p class="exp-content"><i>May. 2018 - Sept. 2018. </i>Helped build up the continuous integration pipeline of products.</p>
                        </li>
                    </ul>
                </div>
                <div id="awards">
                    <h2>Competitions & Awards</h2>
                    <ul>
                        <li>
                            <p class="exp-head">NeurIPS Scholar Award</p>
                            <p class="exp-content">2025.</p>
                        </li>
                        <li>
                            <p class="exp-head">OSU CSE Research Staff Award</p>
                            <p class="exp-content">2025.</p>
                        </li>
                        <li>
                            <p class="exp-head">AAAI Scholarship</p>
                            <p class="exp-content">2024.</p>
                        </li>
                        <li>
                            <p class="exp-head">ActivityNet Temporal Action Localization Challenge</p>
                            <p class="exp-content"><i>Third Place. </i>CVPR Workshop 2022.</p>
                        </li>
                        <li>
                            <p class="exp-head">SoccerNet Challenge</p>
                            <p class="exp-content"><i>Third Place. </i>CVPR Workshop 2022.</p>
                        </li>
                        <li>
                            <p class="exp-head">AICity Challenge</p>
                            <p class="exp-content"><i>First Place. </i>CVPR Workshop 2021.</p>
                        </li>
                        <li>
                            <p class="exp-head">National AI Challenge</p>
                            <p class="exp-content"><i>Second Prize. </i>2020.</p>
                        </li>
                        <li>
                            <p class="exp-head">Visual Domain Adaptation Challenge</p>
                            <p class="exp-content"><i>First Place. </i>ECCV Workshop 2020.</p>
                        </li>
                        <li>
                            <p class="exp-head">Robocup Montreal</p>
                            <p class="exp-content"><i>First Place. </i>2018.</p>
                        </li>
                    </ul>
                </div>
                <div id="service">
                    <h2>Academic Service</h2>
                    <ul>
                        <li>
                            <p class="exp-head">Workshop Organization</p>
                            <p class="exp-content"><i>Lead organizer. </i><a href="https://imageomics.github.io/Imageomics-NeurIPS-2025/">Third Workshop on Imageomics</a> @ NeurIPS2025</p>
                            <p class="exp-content"><i>Co-organizer. </i><a href="https://sites.google.com/view/imageomics-aaai-25/">Second Workshop on Imageomics</a> @ AAAI2025</p>
                            <p class="exp-content"><i>Co-organizer. </i><a href="https://hdr-ecosystem.github.io/hdr-ad-challenge-webpage/aaai-workshop2024.html">Anomaly Detection in Scientific Domains Workshop</a> @ AAAI2025</p>
                            <p class="exp-content"><i>PC Member. </i><a href="https://sites.google.com/view/dd-cvpr2024/home">First Workshop on Dataset Distillation</a> @ CVPR2024 </p>
                        </li>
                        <li>
                            <p class="exp-head">Conference Area Chair</p>
                            <p class="exp-content">ICLR 2026, ICML 2026</p>
                        </li>
                        <li>
                            <p class="exp-head">Conference Reviewer</p>
                            <p class="exp-content">CVPR, ICCV, ECCV, ICLR, ICML, NeurIPS, ACMMM, WACV, ACCV</p>
                        </li>
                        <li>
                            <p class="exp-head">Journal Reviewer</p>
                            <p class="exp-content">PNAS, IEEE TPAMI, PR, CVIU, IEEE TCSVT</p>
                        </li>
                    </ul>
                </div>
                <div id="other-info">
                    <h2>Other Information</h2>
                    <ul>
                        <li>
                            <p><b>President</b>, Student AI Association of Zhejiang University, Aug. 2020 - Jun. 2021</p>
                        </li>
                        <li>
                            <p>Part of my photography <a href="https://unsplash.com/@ypbehere">works</a>.</p>
                        </li>
                    </ul>
                </div>
                <div id="contact">
                    <h2>Contact Me</h2>
                    <p>You are welcome to contact me via Email.</p>
                </div>
            </div>
        </div>

        <div class="visitor">
            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=szeRYJgXin9srYykakESD1C25nuaavZ8st4jGt2CCQM&cl=ffffff&w=a"></script>
        </div>

        <div id="tail">
            <div id="tail-glass">
                Written by Jianyang Gu & CodeX
            </div>
        </div>
    </div>
    <div style=" height:25px " ></div>
</body>
<footer>
</footer>
